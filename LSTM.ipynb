{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies"
      ],
      "metadata": {
        "id": "yGHtvKzlTiVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "Txah9bcaTlZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "LDLfr7r_UIvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df = pd.read_csv('/content/drive/MyDrive/10_Year_Historical_Scaled.csv')"
      ],
      "metadata": {
        "id": "nm_KEQNaUKcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Creation and Train-Test Split"
      ],
      "metadata": {
        "id": "qtvqK5p2Tej7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNhtrYnvTMBR"
      },
      "outputs": [],
      "source": [
        "def create_dataset(data, lookback, prediction_horizon):\n",
        "    n_samples = len(data) - lookback - prediction_horizon\n",
        "    if n_samples <= 0:\n",
        "        return np.empty((0, lookback, data.shape[1] - 1)), np.empty((0, prediction_horizon))\n",
        "\n",
        "    X = np.empty((n_samples, lookback, data.shape[1] - 1), dtype=np.float32)\n",
        "    Y = np.empty((n_samples, prediction_horizon), dtype=np.float32)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        X[i] = data[i:(i + lookback), 1:]\n",
        "        Y[i] = data[(i + lookback):(i + lookback + prediction_horizon), 0]\n",
        "\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_stock_data(df, lookback, prediction_horizon):\n",
        "    tickers = df['Ticker'].unique()\n",
        "    train_x_list, train_y_list, test_x_list, test_y_list = [], [], [], []\n",
        "\n",
        "    total_train_samples, total_test_samples = 0, 0\n",
        "\n",
        "    for ticker in tickers:\n",
        "        stock_data = df[df['Ticker'] == ticker].copy()\n",
        "        stock_data.drop(columns=['Ticker', 'Date'], inplace=True)\n",
        "\n",
        "        train_split_len = int(len(stock_data) * 0.8)\n",
        "        train_data = stock_data.values[:train_split_len]\n",
        "        test_data = stock_data.values[train_split_len:]\n",
        "\n",
        "        usable_train_samples = len(train_data) - lookback - prediction_horizon\n",
        "        usable_test_samples = len(test_data) - lookback - prediction_horizon\n",
        "\n",
        "        print(f\"Ticker: {ticker}, Train rows: {len(train_data)}, Test rows: {len(test_data)}\")\n",
        "        print(f\"Ticker: {ticker}, Usable train samples: {usable_train_samples}, Usable test samples: {usable_test_samples}\")\n",
        "\n",
        "        train_x, train_y = create_dataset(train_data, lookback, prediction_horizon)\n",
        "        test_x, test_y = create_dataset(test_data, lookback, prediction_horizon)\n",
        "\n",
        "        total_train_samples += train_x.shape[0]\n",
        "        total_test_samples += test_x.shape[0]\n",
        "\n",
        "        train_x_list.append(train_x)\n",
        "        train_y_list.append(train_y)\n",
        "        test_x_list.append(test_x)\n",
        "        test_y_list.append(test_y)\n",
        "\n",
        "        del train_data, test_data\n",
        "        gc.collect()\n",
        "\n",
        "    train_x = np.concatenate(train_x_list, axis=0)\n",
        "    train_y = np.concatenate(train_y_list, axis=0)\n",
        "    test_x = np.concatenate(test_x_list, axis=0)\n",
        "    test_y = np.concatenate(test_y_list, axis=0)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Total train samples: {total_train_samples}, Total test samples: {total_test_samples}\")\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n"
      ],
      "metadata": {
        "id": "kMjQtG30TvB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback = 60\n",
        "prediction_horizon = 7\n",
        "\n",
        "train_x, train_y, test_x, test_y = process_stock_data(scaled_df, lookback, prediction_horizon)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Training data shape: X={train_x.shape}, Y={train_y.shape}\")\n",
        "print(f\"Testing data shape: X={test_x.shape}, Y={test_y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDa_ivTeT_hZ",
        "outputId": "497abf8d-8429-47c7-8a39-597092079355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train samples: 2490880, Total test samples: 559360\n",
            "Training data shape: X=(2490880, 60, 5), Y=(2490880, 7)\n",
            "Testing data shape: X=(559360, 60, 5), Y=(559360, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Define LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=32):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, prediction_horizon)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # Take the last time step\n",
        "        return out\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_x_tensor = torch.FloatTensor(train_x)\n",
        "train_y_tensor = torch.FloatTensor(train_y)\n",
        "test_x_tensor = torch.FloatTensor(test_x)\n",
        "test_y_tensor = torch.FloatTensor(test_y)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "input_size = train_x.shape[2]  # Number of features\n",
        "model = LSTMModel(input_size=input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "batch_size = 32\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = len(train_x_tensor) // batch_size\n",
        "\n",
        "    # Progress bar\n",
        "    with tqdm(total=num_batches, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
        "        for i in range(0, len(train_x_tensor), batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            x_batch = train_x_tensor[i:i + batch_size]\n",
        "            y_batch = train_y_tensor[i:i + batch_size]\n",
        "\n",
        "            # Forward pass\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / num_batches\n",
        "    elapsed_time = time.time() - start_time\n",
        "    time_left = (elapsed_time / (epoch + 1)) * (num_epochs - (epoch + 1))\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Avg Loss: {avg_epoch_loss:.4f}, Time Elapsed: {elapsed_time:.2f}s, Estimated Time Left: {time_left:.2f}s')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_predictions = model(test_x_tensor)\n",
        "    test_loss = criterion(test_predictions, test_y_tensor)\n",
        "\n",
        "print(f'Test Loss: {test_loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWD1udTdW-hC",
        "outputId": "7e2af872-c3a2-46da-b21d-8f285a875de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/200: 100%|██████████| 77840/77840 [10:14<00:00, 126.69batch/s, loss=1.04e+13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Avg Loss: 3453230395128.1934, Time Elapsed: 614.40s, Estimated Time Left: 122265.75s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/200:   4%|▍         | 3261/77840 [00:26<13:18, 93.40batch/s, loss=1.74e+10]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvnYdAYNhjOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}